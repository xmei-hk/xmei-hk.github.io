<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>HAMF Project Page</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/logo.jpg"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <img src="static/images/logo.png" alt="Logo" style="width: 300px; height: auto;"> -->
            <!-- <h1 class="title is-1 publication-title"><span style="color:blue">L</span>ook back the <span style="color:blue">H</span>istory and <span style="color:blue">P</span>lan for the <span style="color:blue">F</span>uture in Autonomous Driving</h1> -->
            <h1 class="title is-1 publication-title"><span style="color:royalblue">HAMF</span>: A <span style="color:royalblue">H</span>ybrid <span style="color:royalblue">A</span>ttention-<span style="color:royalblue">M</span>amba Framework for Joint Scene Context Understanding and <span style="color:royalblue">F</span>uture Motion Representation Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=PqpXJZQAAAAJ&hl=en" target="_blank">Xiaodong MEI</a><sup>1</sup>,</span>
                </span>
                <span class="author-block"></span>
                <a href="https://chantsss.github.io/" target="_blank">Sheng WANG</a><sup>1</sup>,</span>
                </span>
                  <span class="author-block"></span>
                    <a href="https://jchengai.github.io/" target="_blank">Jie CHENG</a><sup>1</sup>,</span>
                  </span>
                  <span class="author-block"></span>
                    <a href="https://scholar.google.com/citations?user=8CwcGr4AAAAJ&hl=zh-CN" target="_blank">Yingbing CHEN</a><sup>1</sup>,</span>
                  </span>
                  <!-- </span> <br> -->
                  <!-- next row -->
                  <span class="author-block"></span>
                    <a href="https://www.danxurgb.net/" target="_blank">Dan XU</a><sup>1,†</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>HKUST</span>
                    <span class="eql-cntrb"><small><br><sup>†</sup>Indicates Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://scholar.google.com/citations?user=PqpXJZQAAAAJ&hl=en" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="https://arxiv.org/abs/2411.17253" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/chantsss/LHPF" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://scholar.google.com/citations?user=PqpXJZQAAAAJ&hl=en" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->

<!-- End teaser video -->


<!-- Overview -->
<section class="section hero">
  <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered" style="background: #ffffff">
          <div class="column is-four-fifths">
              <h2 class="title is-3">Overview</h2>
              <div class="content has-text-justified">
          <div class="content has-text-justified">
      </div>
  </div>


<!-- video -->
<hr> <div class="container-gif">

  <div class="item">
      <video src="static/videos/overview.mp4" alt="mp1_1" controls></video>
      <figcaption>Overview of the proposed HAMF.</figcaption>
  </div>
<!-- <p class="gif-caption"><strong>An aggressive overtaking behavior.</strong> The ego vehicle
changes lanes to a neighboring lane at first, accelerates past
vehicles in the original lane, and successfully completes the overtaking. This scenario highlights the
ability of our method to make timely lane changes and execute aggressive overtaking maneuvers effectively.</p> -->

</section>
<!-- End Overview -->

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered" style="background: #ffffff">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            <strong>Motion forecasting</strong> represents a critical challenge in autonomous driving systems, requiring accurate prediction of surrounding agents' future trajectories. While existing approaches predict future motion states with the extracted scene context feature from historical agent trajectories and road layouts, they suffer from the information degradation during the scene feature encoding. To address the limitation, we propose <strong>HAMF, a novel motion forecasting framework that learns future motion representations with the scene context encoding jointly, to coherently combine the scene understanding and future motion state prediction </strong>.
            We first embed the observed agent states and map information into 1D token sequences, together with the target multi-modal future motion features as a set of learnable tokens. Then we design a unified Attention-based encoder, which synergistically combines self-attention and cross-attention mechanisms to model the scene context information and aggregate future motion features jointly. Complementing the encoder, we implement the Mamba module in the decoding stage to further preserve the consistency and correlations among the learned future motion representations, to generate the accurate and diverse final trajectories.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Motivation -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered"style="background: #F2F2F2">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <!-- <div class="columns">
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/first_img_page_1.png" alt="Motivation Image 1" style="height: 400px;"> -->
              <!-- <p><small><strong>Figure 1. Planning with and
                without historical intentions. </small></strong></p> -->
              <!-- <figcaption>Figure 1. Planning with and
                without historical intentions.</figcaption>
            </figure>
          </div>
          <div class="column is-half has-text-centered">
            <figure class="image">
              <img src="static/images/planning_comparison_new_page_1.png" alt="Motivation Image 2" style="height: 400px;">
              <figcaption>Figure 2. Planning Stream Demonstration.</figcaption> -->
              <!-- <p><small><strong>Figure 2. Planning Stream Demonstration.</small></strong></p> -->
            <!-- </figure>
          </div>
        </div>
        <p gif-caption> We compare two imitation learning planners for autonomous driving, focusing on the role of historical intentions. As shown in Figure 1, while both planners perform well initially, the planner without historical context diverges over time, whereas the planner with historical embeddings maintains consistent accuracy. Figure 2 illustrates the continuity and evolution of driving behaviors, emphasizing the importance of incorporating historical context for stable real-world planning.</p>
      <br> -->
        <div class="content has-text-justified">
          <p class="text-center">
              <img src="./static/images/model_v.png" class="center">
          </p>
        </div>
        <p gif-caption> <strong>The left part</strong> presents the input embedding module with an intersection driving scenario. The ground truth future trajectory is shown with the gradient pink line for illustration purposes, which is not used in the input. The historical trajectories and surrounding map are embedded and combined as initial scene tokens, then concatenated with the initial future motion tokens for the input of the unified encoder. <strong>The middle part</strong> denotes the encoding process within the l-th encoder layer. The dashed lines represent the input for the subsequent encoding layer. With several iterations in the encoder, the learned future motion tokens are obtained and decoded with the Mamba block and multi-layer MLPs to generate the final prediction, shown in <strong>the right part</strong>. </p>
      </div>

    </div>
  </div>
</section>
<!-- End Motivation -->

<!-- Method -->
  <!-- <section class="section hero">
    <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered" style="background: #F2F2F2">
        <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
            <p class="text-center">
                <img src="./static/images/pipeline_page_1.png" class="center">
            </p>
            <p>
              A stack of planners generates historical planning embeddings at each time step, which are stored in a historical intention pool and combined with reference line queries. Spatio-temporal queries are then aggregated and processed using self and cross attention with scene context as keys and values. Finally, the current planning embedding is passed through a multi-layer perceptron to generate future trajectories and scores.
            </p>
        </div>
        </div>
    </div>
    </div>
</section> -->
<!-- End Method -->

<!-- Main results -->
<section class="section hero">
  <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered" style="background: #F2F2F2">
          <div class="column is-four-fifths">
              <h2 class="title is-3">Evaluations on Argoverse 2 benchmark</h2>
              <div class="content has-text-justified">
          <div class="content has-text-justified">
          <p class="text-center">
              <img src="./static/images/table_av2.png" class="center">
          </p>
          <!-- <p gif-caption>
            <strong>(a) An aggressive overtaking behavior.</strong> The ego vehicle
            changes lanes to a neighboring lane at t1, accelerates past
            vehicles in the original lane at t2, and successfully com-
            pletes the overtaking by t3. This scenario highlights the
            ability of our method to make timely lane changes and exe-
            cute aggressive overtaking maneuvers effectively.
        </p> -->
      </div>
  </div>
  </section>

<!-- More results -->
<section class="section hero">
  <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered" style="background: #ffffff">
          <div class="column is-four-fifths">
              <h2 class="title is-3">More Visualization Results</h2>
              <div class="content has-text-justified">
          <div class="content has-text-justified">
          <p class="text-center">
              <img src="./static/images/result_1.png" class="center">
          </p>
          <p class="text-center">
            <img src="./static/images/result_2.png" class="center">
        </p>
          <p gif-caption>
            <strong>Qualitative results on the challenging scenarios of AV2 validation set</strong>. Surrounding agents are represented by the bounding boxes in purple and the focal agent in yellow. The line in gradient pink indicates the ground truth and the line in deep blue indicates the multi-modal predicted trajectory.
        </p>
      </div>
  </div>
  </section>

<!-- End Case study -->

<!-- More scenarios -->

<!-- End More scenarios -->

<!-- Image carousel -->
<!-- End image carousel -->

<!-- Youtube video -->
<!-- End youtube video -->

<!-- Video carousel -->
<!-- End video carousel -->

<!-- Paper poster -->
<!--End paper poster -->

<script type="text/javascript">
  document.addEventListener('DOMContentLoaded', function() {
      var videos = document.querySelectorAll('video');
      videos.forEach(function(video) {
          video.autoplay = true;
          video.controls = true;
          video.muted = true;
          video.loop = true;
          video.playsInline = true;
          video.playbackRate = 2.0;
          video.addEventListener('loadedmetadata', function() {
              this.playbackRate = 2.0;
          });
      });
  });
</script>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>The arxiv version is ready.
  </code></pre>
    </div>
</section>
<!--End BibTex citation -->

<!-- Future work -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Future work</h2>
    <pre><code>
    The proposed model can be extended as the learning-based motion planner, and integrated with 
    the perception module as an end-to-end autonomous driving framework. 
    The extension work is on-going. Please stay tuned for the further update!

</code></pre>
  </div>
</section>
<!--End Future work -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            </a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
