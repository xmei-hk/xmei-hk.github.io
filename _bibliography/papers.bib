---
---

@string{aps = {American Physical Society,}}
@string{iros = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems}}
@string{icra = {Proceedings of the IEEE International Conference on Robotics and Automation}}
@string{robio = {Proceedings of the IEEE International Conference on Robotics and Biomimetics}}
@string{ram = {IEEE Robotics and Automation Magazine}}
@string{ral = {IEEE Robotics and Automation Letters}}
@string{tits = {IEEE Transactions on Intelligent Transportation Systems}}

@INPROCEEDINGS{HAMF,
  abbr={IROS 2025},
  title={HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context Understanding and Future Motion Representation Learning},
  booktitle={Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  author={Mei, Xiaodong and Wang, Sheng and Cheng, Jie and Chen, Yingbing and Xu, Dan}, 
  year={2025},
  volume={},
  number={},
  selected={true},
  abstract={Motion forecasting represents a critical challenge in autonomous driving systems, requiring accurate prediction of surrounding agents' future trajectories. While existing approaches predict future motion states with the extracted scene context feature from historical agent trajectories and road layouts, they suffer from the information degradation during the scene feature encoding. To address the limitation, we propose HAMF, a novel motion forecasting framework that learns future motion representations with the scene context encoding jointly, to coherently combine the scene understanding and future motion state prediction. We first embed the observed agent states and map information into 1D token sequences, together with the target multi-modal future motion features as a set of learnable tokens. Then we design a unified Attention-based encoder, which synergistically combines self-attention and cross-attention mechanisms to model the scene context information and aggregate future motion features jointly. Complementing the encoder, we implement the Mamba module in the decoding stage to further preserve the consistency and correlations among the learned future motion representations, to generate the accurate and diverse final trajectories. Extensive experiments on Argoverse 2 benchmark demonstrate that our hybrid Attention-Mamba model achieves state-of-the-art motion forecasting performance with the simple and lightweight architecture.},
  bibtex_show={true},
  preview={HAMF.png}
}

@INPROCEEDINGS{mae,
  abbr={ICCV 2023},
  author={Cheng, Jie and Mei, Xiaodong and Liu, Ming},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision}, 
  title={Forecast-MAE: Self-supervised Pre-training for Motion Forecasting with Masked Autoencoders}, 
  year={2023},
  volume={5},
  number={},
  pages={8679--8689},
  selected={true},
  abstract={This study explores the application of self-supervised learning (SSL) to the task of motion forecasting, an area that has not yet been extensively investigated despite the widespread success of SSL in computer vision and natural language processing. To address this gap, we introduce Forecast-MAE, an extension of the mask autoencoders framework that is specifically designed for self-supervised learning of the motion forecasting task. Our approach includes a novel masking strategy that leverages the strong interconnections between agents' trajectories and road networks, involving complementary masking of agents' future or history trajectories and random masking of lane segments. Our experiments on the challenging Argoverse 2 motion forecasting benchmark show that Forecast-MAE, which utilizes standard Transformer blocks with minimal inductive bias, achieves competitive performance compared to state-of-the-art methods that rely on supervised learning and sophisticated designs. Moreover, it outperforms the previous self-supervised learning method by a significant margin. Code is available at https://github. com/jchengai/forecast-mae.},
  code={https://github.com/jchengai/forecast-mae},
  preview={forecast-mae.png},
  bibtex_show={true},
  doi={10.1109/ICCV51070.2023.00797}}

@INPROCEEDINGS{HGCN-GJS,
  abbr={IROS 2022},
  author={Yuying, Chen* and Congcong, Liu* and Xiaodong, Mei*, and Bertram E., Shi and Ming, Liu},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), (*indicates equal contribution)}, 
  title={HGCN-GJS: Hierarchical graph convolutional network with groupwise joint sampling for trajectory prediction}, 
  year={2022},
  volume={},
  number={},
  pages={13400-13405},
  selected={true},
  abstract={Pedestrian trajectory prediction is of great importance for downstream tasks, such as autonomous driving and mobile robot navigation. Realistic models of the social interactions within the crowd is crucial for accurate pedestrian trajectory prediction. However, most existing methods do not capture group level interactions well, focusing only on pairwise interactions and neglecting group-wise interactions. In this work, we propose a hierarchical graph convolutional network, HGCN-GJS, for trajectory prediction which well leverages group level interactions within the crowd. Furthermore, we introduce a joint sampling scheme that captures co-dependencies between pedestrian trajectories during trajectory generation. Based on group information, this scheme ensures that generated trajectories within each group are consistent with each other, but enables different groups to act more independently. We demonstrate that our proposed network achieves state of the art performance on all datasets we have considered.},
  preview={HGCN-GJS.png},
  bibtex_show={true},
  doi={10.1109/IROS47612.2022.9981037}}

@INPROCEEDINGS{Drift,
  abbr={RAL&ICRA 2020},
  author={Cai*, Peide and Mei*, Xiaodong and Tai, Lei and Sun, Yuxiang and Liu, Ming},
  booktitle={IEEE Robotics and Automation Letters (*indicates equal contribution)}, 
  title={High-speed autonomous drifting with deep reinforcement learning}, 
  year={2020},
  volume={5},
  number={2},
  pages={1247--1254},
  selected={true},
  abstract={Drifting is a complicated task for autonomous vehicle control. Most traditional methods in this area are based on motion equations derived by the understanding of vehicle dynamics, which is difficult to be modeled precisely. We propose a robust drift controller without explicit motion equations, which is based on the latest model-free deep reinforcement learning algorithm soft actor-critic. The drift control problem is formulated as a trajectory following task, where the error-based state and reward are designed. After being trained on tracks with different levels of difficulty, our controller is capable of making the vehicle drift through various sharp corners quickly and stably in the unseen map. The proposed controller is further shown to have excellent generalization ability, which can directly handle unseen vehicle types with different physical properties, such as mass, tire friction, etc.},
  website={https://sites.google.com/view/autonomous-drifting-with-drl/},
  code={https://github.com/caipeide/drift_drl},
  preview={drift.png},
  bibtex_show={true},
  doi={10.1109/LRA.2020.2967299}}

@INPROCEEDINGS{rethinking,
  abbr={ICRA 2024},
  title={Rethinking imitation-based planners for autonomous driving},
  author={Cheng, Jie and Chen, Yingbing and Mei, Xiaodong and Yang, Bowen and Li, Bo and Liu, Ming},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  pages={14123--14130},
  year={2024},
  organization={IEEE},
  preview={rethink.png},
  website={https://jchengai.github.io/planTF},
  bibtex_show={true},
  selected={true},
  code={https://github.com/jchengai/planTF},
  doi={10.1109/ICRA57147.2024.10611364},
  abstract={In recent years, imitation-based driving planners have reported considerable success. However, due to the absence of a standardized benchmark, the effectiveness of various designs remains unclear. The newly released nuPlan addresses this issue by offering a large-scale real-world dataset and a standardized closed-loop benchmark for equitable comparisons. Utilizing this platform, we conduct a comprehensive study on two fundamental yet underexplored aspects of imitation-based planners: the essential features for ego planning and the effective data augmentation techniques to reduce compounding errors. Furthermore, we highlight an imitation gap that has been overlooked by current learning systems. Finally, integrating our findings, we propose a strong baseline model—PlanTF. Our results demonstrate that a well-designed, purely imitation-based planner can achieve highly competitive performance compared to state-of-the-art methods involving hand-crafted rules and exhibit superior generalization capabilities in long-tail cases. Our models and benchmarks are publicly available.}
}  

@INPROCEEDINGS{xin2024generictrajectoryplanningmethod,
  abbr={arxiv 2025},
  title={NetRoller: Interfacing General and Specialized Models for End-to-End Autonomous Driving}, 
  author={Ren Xin and Hongji Liu and Xiaodong Mei and Wenru Liu and Maosheng Ye and Zhili Chen and Jun Ma},
  booktitle={submission},
  year={2025},
  eprint={2506.14589},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2506.14589}, 
  abstract={Integrating General Models (GMs) such as Large Language Models (LLMs), with Specialized Models (SMs) in autonomous driving tasks presents a promising approach to mitigating challenges in data diversity and model capacity of existing specialized driving models. However, this integration leads to problems of asynchronous systems, which arise from the distinct characteristics inherent in GMs and SMs. To tackle this challenge, we propose NetRoller, an adapter that incorporates a set of novel mechanisms to facilitate the seamless integration of GMs and specialized driving models. Specifically, our mechanisms for interfacing the asynchronous GMs and SMs are organized into three key stages. NetRoller first harvests semantically rich and computationally efficient representations from the reasoning processes of LLMs using an early stopping mechanism, which preserves critical insights on driving context while maintaining low overhead. It then applies learnable query embeddings, nonsensical embeddings, and positional layer embeddings to facilitate robust and efficient cross-modality translation. At last, it employs computationally efficient Query Shift and Feature Shift mechanisms to enhance the performance of SMs through few-epoch fine-tuning. Based on the mechanisms formalized in these three stages, NetRoller enables specialized driving models to operate at their native frequencies while maintaining situational awareness of the GM. Experiments conducted on the nuScenes dataset demonstrate that integrating GM through NetRoller significantly improves human similarity and safety in planning tasks, and it also achieves noticeable precision improvements in detection and mapping tasks for end-to-end autonomous driving.},
  selected={true},
  bibtex_show={true},
  code={https://github.com/Rex-sys-hk/NetRoller},
  arxiv={2506.14589},
  preview={NetROller.png}
  }




@misc{LHPF,
      abbr={arxiv 2025},
      title={LHPF: Look back the History and Plan for the Future in Autonomous Driving}, 
      author={Sheng Wang and Yao Tian and Xiaodong Mei and Ge Sun and Jie Cheng and Fulong Ma and Pedro V. Sander and Junwei Liang},
      year={2024},
      arxiv={2411.17253},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      booktitle={submission},
      selected={true},
      website={https://chantsss.github.io/LHPF/},
      code={https://github.com/chantsss/LHPF},
      bibtex_show={true},
      preview={LHPF.png},
      url={https://arxiv.org/abs/2411.17253}, 
      abstract={Decision-making and planning in autonomous driving critically reflect the safety of the system, making effective planning imperative. Current imitation learning-based planning algorithms often merge historical trajectories with present observations to predict future candidate paths. However, these algorithms typically assess the current and historical plans independently, leading to discontinuities in driving intentions and an accumulation of errors with each step in a discontinuous plan. To tackle this challenge, this paper introduces LHPF, an imitation learning planner that integrates historical planning information. Our approach employs a historical intention aggregation module that pools historical planning intentions, which are then combined with a spatial query vector to decode the final planning trajectory. Furthermore, we incorporate a comfort auxiliary task to enhance the human-like quality of the driving behavior. Extensive experiments using both real-world and synthetic data demonstrate that LHPF not only surpasses existing advanced learning-based planners in planning performance but also marks the first instance of a purely learning-based planner outperforming the expert. Additionally, the application of the historical intention aggregation module across various backbones highlights the considerable potential of the proposed method. The code will be made publicly available.},
      
}



@INPROCEEDINGS{POP,
  abbr={ICRA 2024},
  author={Wang, Sheng and Chen, Yingbing and Cheng, Jie and Mei, Xiaodong and Xin, Ren and others},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Improving Autonomous Driving Safety with POP: A Framework for Accurate Partially Observed Trajectory Predictions}, 
  year={2024},
  volume={},
  number={},
  pages={14450-14456},
  abstract={Accurate trajectory prediction is crucial for safe and efficient autonomous driving, 
  but handling partial observations presents significant challenges. To address this, 
  we propose a novel trajectory prediction framework called Partial Observations Prediction (POP) for congested urban road scenarios. 
  The framework consists of two key stages: self-supervised learning (SSL) and feature distillation. 
  POP first employs SLL to help the model learn to reconstruct history representations, 
  and then utilizes feature distillation as the fine-tuning task to transfer knowledge from the teacher model, 
  which has been pre-trained with complete observations, to the student model, which has only few observations. 
  POP achieves comparable results to topperforming methods in open-loop experiments and outperforms the baseline method in closed-loop simulations, 
  including safety metrics. Qualitative results illustrate the superiority of POP in providing reasonable and safe trajectory predictions.},
  pdf={https://ieeexplore.ieee.org/abstract/document/10610154},
  website={https://chantsss.github.io/POP/},
  code={https://github.com/chantsss/POP-CODE},
  bibtex_show={true},
  keywords={Accuracy;Roads;Self-supervised learning;Predictive models;Trajectory;History;Task analysis},
  doi={10.1109/ICRA57147.2024.10610154}}


@INPROCEEDINGS{FCUS,
  abbr={ROBIO 2023},
  author={Wang, Sheng and Xin, Ren and Cheng, Jie and Xiaodong, Mei and Ming Liu},
  booktitle={2023 IEEE International Conference on Robotics and Biomimetics (ROBIO)}, 
  title={FCUS: Traffic Rule-Aware Vehicle Trajectory Forecasting Using Continuous Unlikelihood and Signal Temporal Logic Feature}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  pdf={https://ieeexplore.ieee.org/abstract/document/10354968},
  website={https://chantsss.github.io/FCUS/},
  code={https://github.com/chantsss/FCUS},
  bibtex_show={true},
  keywords={Biological system modeling;Neural networks;Predictive models;Trajectory;Safety;Forecasting;Task analysis},
  doi={10.1109/ROBIO58561.2023.10354968}}

@ARTICLE{RAM,
  abbr={RAM 2024},
  author={Chen, Yingbing and Cheng, Jie and Wang, Sheng and Hongji, Liu and Xiaodong, Mei and Xiaoyang, Yan and Mingkai, Tang and Ge, Sun and others},
  journal={IEEE Robotics & Automation Magazine}, 
  title={Enhancing Campus Mobility: Achievements and Challenges of the Snow Lion Autonomous Shuttle}, 
  year={2024},
  volume={},
  number={},
  pages={2-13},
  arxiv={2401.08939},
  pdf={https://ieeexplore.ieee.org/abstract/document/10623822},
  website={https://chenyingbing.github.io/xueshi_campus_av/},
  bibtex_show={true},
  abstract={In recent years, the rapid evolution of autonomous vehicles (AVs) has reshaped global transportation systems, 
leading to an increase in autonomous shuttle applications in people’s daily lives. Leveraging the accomplishments of our earlier endeavor, 
particularly Hercules [1], an autonomous logistics vehicle for transporting goods, 
we introduce Snow Lion, an autonomous shuttle vehicle specifically designed to transform on-campus transportation, 
providing a safe and efficient mobility solution for students, faculty, and visitors.},
  keywords={Laser radar;Task analysis;Sensors;Point cloud compression;Location awareness;Three-dimensional displays;Planning},
  doi={10.1109/MRA.2024.3433168}}
